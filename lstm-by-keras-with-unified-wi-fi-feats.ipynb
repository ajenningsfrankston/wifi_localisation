{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015257,
     "end_time": "2021-03-05T11:00:45.642040",
     "exception": false,
     "start_time": "2021-03-05T11:00:45.626783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Overview\n",
    "\n",
    "It demonstrates how to utilize [the unified Wi-Fi dataset](https://www.kaggle.com/kokitanisaka/indoorunifiedwifids).<br>\n",
    "The Neural Net model is not optimized, there's much space to improve the score. \n",
    "\n",
    "In this notebook, I refer these two excellent notebooks.\n",
    "* [wifi features with lightgbm/KFold](https://www.kaggle.com/hiro5299834/wifi-features-with-lightgbm-kfold) by [@hiro5299834](https://www.kaggle.com/hiro5299834/)<br>\n",
    " I took some code fragments from his notebook.\n",
    "* [Simple ðŸ‘Œ 99% Accurate Floor Model ðŸ’¯](https://www.kaggle.com/nigelhenry/simple-99-accurate-floor-model) by [@nigelhenry](https://www.kaggle.com/nigelhenry/)<br>\n",
    " I use his excellent work, the \"floor\" prediction.\n",
    "\n",
    "It takes much much time to finish learning. <br>\n",
    "And even though I enable the GPU, it doesn't help. <br>\n",
    "If anybody knows how to make it better, can you please make a comment? <br>\n",
    "\n",
    "Thank you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:00:45.678765Z",
     "iopub.status.busy": "2021-03-05T11:00:45.678024Z",
     "iopub.status.idle": "2021-03-05T11:00:53.223193Z",
     "shell.execute_reply": "2021-03-05T11:00:53.224049Z"
    },
    "papermill": {
     "duration": 7.56848,
     "end_time": "2021-03-05T11:00:53.224420",
     "exception": false,
     "start_time": "2021-03-05T11:00:45.655940",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow_addons.layers import WeightNormalization\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.013025,
     "end_time": "2021-03-05T11:00:53.251239",
     "exception": false,
     "start_time": "2021-03-05T11:00:53.238214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "### options\n",
    "We can change the way it learns with these options. <br>\n",
    "Especialy **NUM_FEATS** is one of the most important options. <br>\n",
    "It determines how many features are used in the training. <br>\n",
    "We have 100 Wi-Fi features in the dataset, but 100th Wi-Fi signal sounds not important, right? <br>\n",
    "So we can use top Wi-Fi signals if we think we need to. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:00:53.281242Z",
     "iopub.status.busy": "2021-03-05T11:00:53.280559Z",
     "iopub.status.idle": "2021-03-05T11:00:53.285759Z",
     "shell.execute_reply": "2021-03-05T11:00:53.286343Z"
    },
    "papermill": {
     "duration": 0.022119,
     "end_time": "2021-03-05T11:00:53.286554",
     "exception": false,
     "start_time": "2021-03-05T11:00:53.264435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# options\n",
    "\n",
    "N_SPLITS = 10\n",
    "\n",
    "SEED = 2021\n",
    "\n",
    "NUM_FEATS = 20 # number of features that we use. there are 100 feats but we don't need to use all of them\n",
    "\n",
    "base_path = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2021-03-05T11:00:53.317471Z",
     "iopub.status.busy": "2021-03-05T11:00:53.316843Z",
     "iopub.status.idle": "2021-03-05T11:00:53.324858Z",
     "shell.execute_reply": "2021-03-05T11:00:53.325417Z"
    },
    "papermill": {
     "duration": 0.024731,
     "end_time": "2021-03-05T11:00:53.325619",
     "exception": false,
     "start_time": "2021-03-05T11:00:53.300888",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    session_conf = tf.compat.v1.ConfigProto(\n",
    "        intra_op_parallelism_threads=1,\n",
    "        inter_op_parallelism_threads=1\n",
    "    )\n",
    "    sess = tf.compat.v1.Session(graph=tf.compat.v1.get_default_graph(), config=session_conf)\n",
    "    tf.compat.v1.keras.backend.set_session(sess)\n",
    "    \n",
    "def comp_metric(xhat, yhat, fhat, x, y, f):\n",
    "    intermediate = np.sqrt(np.power(xhat-x, 2) + np.power(yhat-y, 2)) + 15 * np.abs(fhat-f)\n",
    "    return intermediate.sum()/xhat.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:00:53.357053Z",
     "iopub.status.busy": "2021-03-05T11:00:53.356315Z",
     "iopub.status.idle": "2021-03-05T11:00:53.446993Z",
     "shell.execute_reply": "2021-03-05T11:00:53.447592Z"
    },
    "papermill": {
     "duration": 0.108223,
     "end_time": "2021-03-05T11:00:53.447786",
     "exception": false,
     "start_time": "2021-03-05T11:00:53.339563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_dir = f\"{base_path}/input/indoorunifiedwifids\"\n",
    "train_files = sorted(glob.glob(os.path.join(feature_dir, '*_train.csv')))\n",
    "test_files = sorted(glob.glob(os.path.join(feature_dir, '*_test.csv')))\n",
    "subm = pd.read_csv(f'{base_path}/input/indoor-location-navigation/sample_submission.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:00:53.480987Z",
     "iopub.status.busy": "2021-03-05T11:00:53.480279Z",
     "iopub.status.idle": "2021-03-05T11:01:16.265565Z",
     "shell.execute_reply": "2021-03-05T11:01:16.264957Z"
    },
    "papermill": {
     "duration": 22.804009,
     "end_time": "2021-03-05T11:01:16.265721",
     "exception": false,
     "start_time": "2021-03-05T11:00:53.461712",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f'{feature_dir}/train_all.pkl', 'rb') as f:\n",
    "  data = pickle.load( f)\n",
    "\n",
    "with open(f'{feature_dir}/test_all.pkl', 'rb') as f:\n",
    "  test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:01:16.299075Z",
     "iopub.status.busy": "2021-03-05T11:01:16.298379Z",
     "iopub.status.idle": "2021-03-05T11:01:16.302350Z",
     "shell.execute_reply": "2021-03-05T11:01:16.301741Z"
    },
    "papermill": {
     "duration": 0.022724,
     "end_time": "2021-03-05T11:01:16.302519",
     "exception": false,
     "start_time": "2021-03-05T11:01:16.279795",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# training target features\n",
    "\n",
    "BSSID_FEATS = [f'bssid_{i}' for i in range(NUM_FEATS)]\n",
    "RSSI_FEATS  = [f'rssi_{i}' for i in range(NUM_FEATS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:01:16.385826Z",
     "iopub.status.busy": "2021-03-05T11:01:16.356245Z",
     "iopub.status.idle": "2021-03-05T11:01:20.548253Z",
     "shell.execute_reply": "2021-03-05T11:01:20.548779Z"
    },
    "papermill": {
     "duration": 4.232407,
     "end_time": "2021-03-05T11:01:20.548994",
     "exception": false,
     "start_time": "2021-03-05T11:01:16.316587",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BSSID TYPES: 61206\n",
      "BSSID TYPES: 33042\n"
     ]
    }
   ],
   "source": [
    "# get numbers of bssids to embed them in a layer\n",
    "\n",
    "wifi_bssids = []\n",
    "for i in range(100):\n",
    "    wifi_bssids.extend(data.iloc[:,i].values.tolist())\n",
    "wifi_bssids = list(set(wifi_bssids))\n",
    "\n",
    "wifi_bssids_size = len(wifi_bssids)\n",
    "print(f'BSSID TYPES: {wifi_bssids_size}')\n",
    "\n",
    "wifi_bssids_test = []\n",
    "for i in range(100):\n",
    "    wifi_bssids_test.extend(test_data.iloc[:,i].values.tolist())\n",
    "wifi_bssids_test = list(set(wifi_bssids_test))\n",
    "\n",
    "wifi_bssids_size = len(wifi_bssids_test)\n",
    "print(f'BSSID TYPES: {wifi_bssids_size}')\n",
    "\n",
    "wifi_bssids.extend(wifi_bssids_test)\n",
    "wifi_bssids_size = len(wifi_bssids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:01:20.615374Z",
     "iopub.status.busy": "2021-03-05T11:01:20.614531Z",
     "iopub.status.idle": "2021-03-05T11:01:23.377862Z",
     "shell.execute_reply": "2021-03-05T11:01:23.377306Z"
    },
    "papermill": {
     "duration": 2.814103,
     "end_time": "2021-03-05T11:01:23.378020",
     "exception": false,
     "start_time": "2021-03-05T11:01:20.563917",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "StandardScaler()"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# preprocess\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(wifi_bssids)\n",
    "le_site = LabelEncoder()\n",
    "le_site.fit(data['site_id'])\n",
    "\n",
    "ss = StandardScaler()\n",
    "ss.fit(data.loc[:,RSSI_FEATS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:01:23.433338Z",
     "iopub.status.busy": "2021-03-05T11:01:23.432628Z",
     "iopub.status.idle": "2021-03-05T11:01:55.635672Z",
     "shell.execute_reply": "2021-03-05T11:01:55.635067Z"
    },
    "papermill": {
     "duration": 32.242398,
     "end_time": "2021-03-05T11:01:55.635854",
     "exception": false,
     "start_time": "2021-03-05T11:01:23.393456",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data.loc[:,RSSI_FEATS] = ss.transform(data.loc[:,RSSI_FEATS])\n",
    "for i in BSSID_FEATS:\n",
    "    data.loc[:,i] = le.transform(data.loc[:,i])\n",
    "    data.loc[:,i] = data.loc[:,i] + 1\n",
    "    \n",
    "data.loc[:, 'site_id'] = le_site.transform(data.loc[:, 'site_id'])\n",
    "\n",
    "data.loc[:,RSSI_FEATS] = ss.transform(data.loc[:,RSSI_FEATS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:01:55.675466Z",
     "iopub.status.busy": "2021-03-05T11:01:55.674190Z",
     "iopub.status.idle": "2021-03-05T11:01:58.697308Z",
     "shell.execute_reply": "2021-03-05T11:01:58.697853Z"
    },
    "papermill": {
     "duration": 3.046874,
     "end_time": "2021-03-05T11:01:58.698036",
     "exception": false,
     "start_time": "2021-03-05T11:01:55.651162",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_data.loc[:,RSSI_FEATS] = ss.transform(test_data.loc[:,RSSI_FEATS])\n",
    "for i in BSSID_FEATS:\n",
    "    test_data.loc[:,i] = le.transform(test_data.loc[:,i])\n",
    "    test_data.loc[:,i] = test_data.loc[:,i] + 1\n",
    "    \n",
    "test_data.loc[:, 'site_id'] = le_site.transform(test_data.loc[:, 'site_id'])\n",
    "\n",
    "test_data.loc[:,RSSI_FEATS] = ss.transform(test_data.loc[:,RSSI_FEATS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:01:58.731427Z",
     "iopub.status.busy": "2021-03-05T11:01:58.730682Z",
     "iopub.status.idle": "2021-03-05T11:01:58.741100Z",
     "shell.execute_reply": "2021-03-05T11:01:58.740529Z"
    },
    "papermill": {
     "duration": 0.028142,
     "end_time": "2021-03-05T11:01:58.741272",
     "exception": false,
     "start_time": "2021-03-05T11:01:58.713130",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "site_count = len(data['site_id'].unique())\n",
    "data.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:01:58.777373Z",
     "iopub.status.busy": "2021-03-05T11:01:58.776743Z",
     "iopub.status.idle": "2021-03-05T11:01:58.798624Z",
     "shell.execute_reply": "2021-03-05T11:01:58.798067Z"
    },
    "papermill": {
     "duration": 0.04201,
     "end_time": "2021-03-05T11:01:58.798775",
     "exception": false,
     "start_time": "2021-03-05T11:01:58.756765",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.015023,
     "end_time": "2021-03-05T11:01:58.829586",
     "exception": false,
     "start_time": "2021-03-05T11:01:58.814563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## The model\n",
    "The first Embedding layer is very important. <br>\n",
    "Thanks to the layer, we can make sense of these BSSID features. <br>\n",
    "<br>\n",
    "We concatenate all the features and put them into LSTM. <br>\n",
    "<br>\n",
    "If something is theoritically wrong, please correct me. Thank you in advance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:01:58.863192Z",
     "iopub.status.busy": "2021-03-05T11:01:58.862594Z",
     "iopub.status.idle": "2021-03-05T11:01:58.874830Z",
     "shell.execute_reply": "2021-03-05T11:01:58.875380Z"
    },
    "papermill": {
     "duration": 0.030789,
     "end_time": "2021-03-05T11:01:58.875583",
     "exception": false,
     "start_time": "2021-03-05T11:01:58.844794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(input_data):\n",
    "\n",
    "    # bssid feats\n",
    "    input_dim = input_data[0].shape[1]\n",
    "\n",
    "    input_embd_layer = L.Input(shape=(input_dim,))\n",
    "    x1 = L.Embedding(wifi_bssids_size, 64)(input_embd_layer)\n",
    "    x1 = L.Flatten()(x1)\n",
    "\n",
    "    # rssi feats\n",
    "    input_dim = input_data[1].shape[1]\n",
    "\n",
    "    input_layer = L.Input(input_dim, )\n",
    "    x2 = L.BatchNormalization()(input_layer)\n",
    "    x2 = L.Dense(NUM_FEATS * 64, activation='relu')(x2)\n",
    "\n",
    "    # site\n",
    "    input_site_layer = L.Input(shape=(1,))\n",
    "    x3 = L.Embedding(site_count, 2)(input_site_layer)\n",
    "    x3 = L.Flatten()(x3)\n",
    "\n",
    "\n",
    "    # main stream\n",
    "    x = L.Concatenate(axis=1)([x1, x3, x2])\n",
    "\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.Dropout(0.3)(x)\n",
    "    x = L.Dense(256, activation='relu')(x)\n",
    "\n",
    "    x = L.Reshape((1, -1))(x)\n",
    "    x = L.BatchNormalization()(x)\n",
    "    x = L.LSTM(128, dropout=0.3, recurrent_dropout=0.3, return_sequences=True, activation='relu')(x)\n",
    "    x = L.LSTM(16, dropout=0.1, return_sequences=False, activation='relu')(x)\n",
    "\n",
    "    \n",
    "    output_layer_1 = L.Dense(2, name='xy')(x)\n",
    "    output_layer_2 = L.Dense(1, activation='softmax', name='floor')(x)\n",
    "\n",
    "    model = M.Model([input_embd_layer, input_layer, input_site_layer], \n",
    "                    [output_layer_1, output_layer_2])\n",
    "\n",
    "    model.compile(optimizer=tf.optimizers.Adam(lr=0.001),\n",
    "                  loss='mse', metrics=['mse'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T11:01:58.910478Z",
     "iopub.status.busy": "2021-03-05T11:01:58.909790Z",
     "iopub.status.idle": "2021-03-05T12:01:16.469937Z",
     "shell.execute_reply": "2021-03-05T12:01:16.333142Z"
    },
    "papermill": {
     "duration": 3557.578715,
     "end_time": "2021-03-05T12:01:16.470117",
     "exception": false,
     "start_time": "2021-03-05T11:01:58.891402",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_df = pd.DataFrame()\n",
    "oof = list()\n",
    "predictions = list()\n",
    "\n",
    "oof_x, oof_y, oof_f = np.zeros(data.shape[0]), np.zeros(data.shape[0]), np.zeros(data.shape[0])\n",
    "preds_x, preds_y = 0, 0\n",
    "preds_f_arr = np.zeros((test_data.shape[0], N_SPLITS))\n",
    "\n",
    "for fold, (trn_idx, val_idx) in enumerate(StratifiedKFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED).split(data.loc[:, 'path'], data.loc[:, 'path'])):\n",
    "    X_train = data.loc[trn_idx, BSSID_FEATS + RSSI_FEATS + ['site_id']]\n",
    "    y_trainx = data.loc[trn_idx, 'x']\n",
    "    y_trainy = data.loc[trn_idx, 'y']\n",
    "    y_trainf = data.loc[trn_idx, 'floor']\n",
    "\n",
    "    tmp = pd.concat([y_trainx, y_trainy], axis=1)\n",
    "    y_train = [tmp, y_trainf]\n",
    "\n",
    "    X_valid = data.loc[val_idx, BSSID_FEATS + RSSI_FEATS + ['site_id']]\n",
    "    y_validx = data.loc[val_idx, 'x']\n",
    "    y_validy = data.loc[val_idx, 'y']\n",
    "    y_validf = data.loc[val_idx, 'floor']\n",
    "\n",
    "    tmp = pd.concat([y_validx, y_validy], axis=1)\n",
    "    y_valid = [tmp, y_validf]\n",
    "\n",
    "    model = create_model([X_train.loc[:,BSSID_FEATS], X_train.loc[:,RSSI_FEATS], X_train.loc[:,'site_id']])\n",
    "    model.fit([X_train.loc[:,BSSID_FEATS], X_train.loc[:,RSSI_FEATS], X_train.loc[:,'site_id']], y_train, \n",
    "                validation_data=([X_valid.loc[:,BSSID_FEATS], X_valid.loc[:,RSSI_FEATS], X_valid.loc[:,'site_id']], y_valid), \n",
    "                batch_size=128, epochs=1000,\n",
    "                callbacks=[\n",
    "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=3, verbose=1, min_delta=1e-4, mode='min')\n",
    "                , ModelCheckpoint(f'{base_path}/RNN_{SEED}_{fold}.hdf5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=True, mode='min')\n",
    "                , EarlyStopping(monitor='val_loss', min_delta=1e-4, patience=5, mode='min', baseline=None, restore_best_weights=True)\n",
    "            ])\n",
    "\n",
    "    model.load_weights(f'{base_path}/RNN_{SEED}_{fold}.hdf5')\n",
    "    val_pred = model.predict([X_valid.loc[:,BSSID_FEATS], X_valid.loc[:,RSSI_FEATS], X_valid.loc[:,'site_id']])\n",
    "\n",
    "    oof_x[val_idx] = val_pred[0][:,0]\n",
    "    oof_y[val_idx] = val_pred[0][:,1]\n",
    "    oof_f[val_idx] = val_pred[1][:,0].astype(int)\n",
    "\n",
    "    pred = model.predict([test_data.loc[:,BSSID_FEATS], test_data.loc[:,RSSI_FEATS], test_data.loc[:,'site_id']]) # test_data.iloc[:, :-1])\n",
    "    preds_x += pred[0][:,0]\n",
    "    preds_y += pred[0][:,1]\n",
    "    preds_f_arr[:, fold] = pred[1][:,0].astype(int)\n",
    "\n",
    "    score = comp_metric(oof_x[val_idx], oof_y[val_idx], oof_f[val_idx],\n",
    "                        y_validx.to_numpy(), y_validy.to_numpy(), y_validf.to_numpy())\n",
    "    print(f\"fold {fold}: mean position error {score}\")\n",
    "\n",
    "    break # for demonstration, run just one fold as it takes much time.\n",
    "\n",
    "preds_x /= (fold + 1)\n",
    "preds_y /= (fold + 1)\n",
    "    \n",
    "print(\"*+\"*40)\n",
    "# as it breaks in the middle of cross-validation, the score is not accurate at all.\n",
    "score = comp_metric(oof_x, oof_y, oof_f, data.iloc[:, -5].to_numpy(), data.iloc[:, -4].to_numpy(), data.iloc[:, -3].to_numpy())\n",
    "oof.append(score)\n",
    "print(f\"mean position error {score}\")\n",
    "print(\"*+\"*40)\n",
    "\n",
    "preds_f_mode = stats.mode(preds_f_arr, axis=1)\n",
    "preds_f = preds_f_mode[0].astype(int).reshape(-1)\n",
    "test_preds = pd.DataFrame(np.stack((preds_f, preds_x, preds_y))).T\n",
    "test_preds.columns = subm.columns\n",
    "test_preds.index = test_data[\"site_path_timestamp\"]\n",
    "test_preds[\"floor\"] = test_preds[\"floor\"].astype(int)\n",
    "predictions.append(test_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T12:01:46.892296Z",
     "iopub.status.busy": "2021-03-05T12:01:46.891293Z",
     "iopub.status.idle": "2021-03-05T12:01:46.895141Z",
     "shell.execute_reply": "2021-03-05T12:01:46.894530Z"
    },
    "papermill": {
     "duration": 15.23028,
     "end_time": "2021-03-05T12:01:46.895283",
     "exception": false,
     "start_time": "2021-03-05T12:01:31.665003",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_preds = pd.concat(predictions)\n",
    "all_preds = all_preds.reindex(subm.index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 15.319126,
     "end_time": "2021-03-05T12:02:17.223894",
     "exception": false,
     "start_time": "2021-03-05T12:02:01.904768",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Fix the floor prediction\n",
    "So far, it is not successfully make the \"floor\" prediction part with this dataset. <br>\n",
    "To make it right, we can incorporate [@nigelhenry](https://www.kaggle.com/nigelhenry/)'s [excellent work](https://www.kaggle.com/nigelhenry/simple-99-accurate-floor-model). <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T12:02:47.612691Z",
     "iopub.status.busy": "2021-03-05T12:02:47.612031Z",
     "iopub.status.idle": "2021-03-05T12:02:47.662670Z",
     "shell.execute_reply": "2021-03-05T12:02:47.662079Z"
    },
    "papermill": {
     "duration": 15.268768,
     "end_time": "2021-03-05T12:02:47.662842",
     "exception": false,
     "start_time": "2021-03-05T12:02:32.394074",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "simple_accurate_99 = pd.read_csv('../input/simple-99-accurate-floor-model/submission.csv')\n",
    "\n",
    "all_preds['floor'] = simple_accurate_99['floor'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-03-05T12:03:17.803729Z",
     "iopub.status.busy": "2021-03-05T12:03:17.803066Z",
     "iopub.status.idle": "2021-03-05T12:03:17.886442Z",
     "shell.execute_reply": "2021-03-05T12:03:17.885697Z"
    },
    "papermill": {
     "duration": 15.239501,
     "end_time": "2021-03-05T12:03:17.886632",
     "exception": false,
     "start_time": "2021-03-05T12:03:02.647131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_preds.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 15.069713,
     "end_time": "2021-03-05T12:03:48.049165",
     "exception": false,
     "start_time": "2021-03-05T12:03:32.979452",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "That's it. \n",
    "\n",
    "Thank you for reading all of it.\n",
    "\n",
    "I hope it helps!\n",
    "\n",
    "Please make comments if you found something to point out, insights or suggestions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 3807.504194,
   "end_time": "2021-03-05T12:04:06.940880",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-03-05T11:00:39.436686",
   "version": "2.2.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}